{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyNerualNetwork Class\n",
    "\n",
    "From zero my implementation of feedforward nerual network with basic backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "# Helper functions\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500) # not to run into overflow error\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derived(x):\n",
    "    x = np.clip(x, -100, 100) # not to run into overflow error\n",
    "    return np.exp(x)/(1 + np.exp(x))**2\n",
    "\n",
    "class MyNeuralNetwork():\n",
    "    '''\n",
    "    Activation function in hidden layers is sigmoid. \n",
    "    Output layer is linear for now (regression problems only).\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, architecture):\n",
    "\n",
    "        print(\"Initializing network with architecture:\", architecture)\n",
    "        \n",
    "        self.architecture = architecture  # Array describing network structure, first is input, last is output all others are hidden\n",
    "        self.input_size = architecture[0] # Input features size\n",
    "        self.layers = len(architecture)   # Number of Layers\n",
    "        \n",
    "        self.W = [] # list of weight matrices\n",
    "        self.initialize_W()\n",
    "        \n",
    "    def initialize_W(self):\n",
    "        '''\n",
    "        Initializes weights from N(0, 1/sqrt(input_features_size))\n",
    "        '''\n",
    "        for i in range(self.layers - 1):\n",
    "            tmp_W = np.random.normal(loc = 0, scale = 1/sqrt(self.input_size), size = self.architecture[i:i+2])\n",
    "            self.W.append(tmp_W)\n",
    "    \n",
    "    def feedforward_one_step(self, x, layer):\n",
    "        y = np.dot(x, self.W[layer])\n",
    "        if layer == self.layers - 2:  # Do not apply activation function in output layer calculations\n",
    "            return y\n",
    "        else:                         # Apply activation function\n",
    "            return sigmoid(y)\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        '''For a given input vector column calculate output'''\n",
    "        for layer in range(self.layers - 1):\n",
    "            x = self.feedforward_one_step(x, layer)\n",
    "        return x\n",
    "    \n",
    "    def feedforward_full(self, x):\n",
    "        '''For a given input vector column feature vector for each layer'''\n",
    "        list_x = []\n",
    "        x.shape = (1, x.size)\n",
    "        list_x.append(x)\n",
    "        \n",
    "        for layer in range(self.layers - 1):\n",
    "            x = self.feedforward_one_step(x, layer)\n",
    "            x.shape = (1, x.size)\n",
    "            list_x.append(x)\n",
    "\n",
    "        return list_x\n",
    "    \n",
    "    def backpropagation(self, x, y, alpha = 0.01):\n",
    "        list_x = self.feedforward_full(x) # Ima indekse 0, 1, 2, 3\n",
    "        gradients = []\n",
    "        delta = 1\n",
    "        for layer in range(self.layers - 2, -1, -1):  ## ide 2, 1, 0\n",
    "            new_gradient = np.dot(delta, list_x[layer])\n",
    "            new_gradient = np.transpose(new_gradient)\n",
    "            gradients.append(new_gradient)\n",
    "            \n",
    "            if(layer > 0):\n",
    "                psi = np.dot(list_x[layer-1], self.W[layer-1])\n",
    "                sigm_d = np.eye(psi.size) * sigmoid_derived(psi)\n",
    "                delta = np.dot(np.dot(sigm_d, self.W[layer]), delta)\n",
    "        gradients.reverse()\n",
    "        \n",
    "        error = list_x[-1] - y\n",
    "        # We have gradients, now we just apply them\n",
    "        for i, W in enumerate(self.W):\n",
    "            '''\n",
    "            print(\"Current Weights: \\n\", self.W[i], \n",
    "                  \"\\nNet Estimate\", list_x[-1],\n",
    "                  \"\\nError: \\n\", error,\n",
    "                 \"\\nGradient: \\n\", gradients[i],\n",
    "                 \"\\n Update: \\n\", - alpha * error * gradients[i])\n",
    "            '''\n",
    "            self.W[i] = self.W[i] - alpha * error * gradients[i]\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Case Sarsa, Q-learning and Expected Sarsa Algorithms\n",
    "\n",
    "Implementation of three algorithms for reinforcement learning on simple gridworld problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    ''' Gridworld enviroment dimensions (grid_h, grid_w) with different possible obstacles, teleports.'''\n",
    "    def env_init(self, env_info={}):\n",
    "        self.start_loc = (0, 0)\n",
    "        self.goal_loc = (3, 5)\n",
    "        self.teleport = [(0, 1), (1, 1), (1, 3), (2, 3), (3, 3), (2, 5)]\n",
    "        \n",
    "        self.grid_w = 6\n",
    "        self.grid_h = 4\n",
    "        \n",
    "    def state(self, loc):\n",
    "        return loc[0] * self.grid_w + loc[1]\n",
    "    \n",
    "    def env_start(self):\n",
    "        reward = 0\n",
    "        self.agent_loc = self.start_loc\n",
    "        state = self.state(self.agent_loc)\n",
    "        termination = False\n",
    "        self.reward_state_term = (reward, state, termination)\n",
    "        return self.reward_state_term\n",
    "    \n",
    "    def env_step(self, action):\n",
    "        \n",
    "        if action == 0: # Up action\n",
    "            possible_next_loc = (self.agent_loc[0] - 1, self.agent_loc[1])\n",
    "            if possible_next_loc[0] >= 0:\n",
    "                self.agent_loc = possible_next_loc\n",
    "            else:\n",
    "                pass\n",
    "        elif action == 1: # Left action\n",
    "            possible_next_loc = (self.agent_loc[0], self.agent_loc[1] - 1)\n",
    "            if possible_next_loc[1] >= 0:\n",
    "                self.agent_loc = possible_next_loc\n",
    "            else:\n",
    "                pass\n",
    "        elif action == 2: # Down action\n",
    "            possible_next_loc = (self.agent_loc[0] + 1, self.agent_loc[1])\n",
    "            if possible_next_loc[0] < self.grid_h:\n",
    "                self.agent_loc = possible_next_loc\n",
    "            else:\n",
    "                pass\n",
    "        elif action == 3: # Right action\n",
    "            possible_next_loc = (self.agent_loc[0], self.agent_loc[1] + 1)\n",
    "            if possible_next_loc[1] < self.grid_w:\n",
    "                self.agent_loc = possible_next_loc\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            raise Exception(\"Action not recognised!\")\n",
    "            \n",
    "        reward = -1\n",
    "        terminal = False\n",
    "        \n",
    "        if self.agent_loc == self.goal_loc:\n",
    "            terminal = True\n",
    "        elif self.agent_loc in self.teleport:\n",
    "            self.agent_loc = self.start_loc\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        self.reward_state_term = (reward, self.state(self.agent_loc), terminal)\n",
    "        return self.reward_state_term\n",
    "    \n",
    "    def env_cleanup(self):\n",
    "        self.agent_loc = self.start_loc\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTD0Agent():\n",
    "    def agent_init(self, agent_info = {}):\n",
    "        self.gamma = agent_info.get('gamma') # Discount factor\n",
    "        self.alpha = agent_info.get('alpha') # Step Size\n",
    "        self.epsilon = agent_info.get('epsilon') # E greedy policy\n",
    "        \n",
    "        self.n_actions = agent_info.get('n_actions')\n",
    "        self.n_states = agent_info.get('n_states')\n",
    "        \n",
    "        self.Q = np.random.uniform(size = (self.n_states, self.n_actions))\n",
    "        self.policy = np.zeros((self.n_states, self.n_actions))\n",
    "        \n",
    "    def get_e_greedy_policy(self):\n",
    "        policy = np.zeros(self.Q.shape)\n",
    "        # For each state\n",
    "        for i in range(policy.shape[0]):\n",
    "            policy[i, :] = self.epsilon/self.n_actions\n",
    "            arg_max = self.Q[i, :].argmax()\n",
    "            policy[i, arg_max] += 1 - self.epsilon\n",
    "        return policy\n",
    "        \n",
    "    def get_greedy_policy(self):\n",
    "        policy = np.zeros(self.Q.shape)\n",
    "        # For each state\n",
    "        for i in range(policy.shape[0]):\n",
    "            arg_max = self.Q[i, :].argmax()\n",
    "            policy[i, arg_max] = 1\n",
    "        return policy\n",
    "            \n",
    "    def agent_start(self, state):\n",
    "        # Choose new action\n",
    "        self.policy = self.get_e_greedy_policy()\n",
    "        \n",
    "        action = np.random.choice(range(self.n_actions), p = self.policy[state])\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        return action\n",
    "    \n",
    "    def agent_step(self, reward, state):\n",
    "        pass\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        # Update values when in terminal state for sarsa\n",
    "        target = reward\n",
    "        self.Q[self.last_state, self.last_action] += self.alpha * (target - self.Q[self.last_state, self.last_action])\n",
    "        \n",
    "        \n",
    "    def agent_cleanup(self):\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        \n",
    "    def agent_message(self, message):\n",
    "        if message == 'get_Q':\n",
    "            return self.Q\n",
    "        elif message == 'check_policy':\n",
    "            policy = self.get_greedy_policy()\n",
    "            \n",
    "            print(\"State 0: \", self.get_action_name(policy[0, ]))\n",
    "            print(\"State 6: \", self.get_action_name(policy[6, ]))\n",
    "            print(\"State 12: \", self.get_action_name(policy[12, ]))\n",
    "            print(\"State 13: \", self.get_action_name(policy[13, ]))\n",
    "            print(\"State 14: \", self.get_action_name(policy[14, ]))\n",
    "            print(\"State 8: \", self.get_action_name(policy[8, ]))\n",
    "            print(\"State 2: \", self.get_action_name(policy[2, ]))\n",
    "            print(\"State 3: \", self.get_action_name(policy[3, ]))\n",
    "            print(\"State 4: \", self.get_action_name(policy[4, ]))\n",
    "            print(\"State 10: \", self.get_action_name(policy[10, ]))\n",
    "            print(\"State 16: \", self.get_action_name(policy[16, ]))\n",
    "            print(\"State 22: \", self.get_action_name(policy[22, ]))\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"SARSA Agent message not understood!\")\n",
    "    \n",
    "    def get_action_name(self, x):\n",
    "        ind = x.argmax()\n",
    "        if ind == 0:\n",
    "            return 'Up'\n",
    "        elif ind == 1:\n",
    "            return 'Left'\n",
    "        elif ind == 2:\n",
    "            return 'Down'\n",
    "        elif ind == 3:\n",
    "            return 'Right'\n",
    "        else: \n",
    "            'Other'  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent(BaseTD0Agent):\n",
    "    def agent_step(self, reward, state):\n",
    "        # Update Policy\n",
    "        self.policy = self.get_e_greedy_policy()\n",
    "        \n",
    "        # Choose new actions: SARSA -> last_state, last_action, reward, state, action\n",
    "        action = np.random.choice(range(self.n_actions), p = self.policy[state])\n",
    "        \n",
    "        # Update rule for sarsa\n",
    "        target = reward + self.gamma * self.Q[state, action]\n",
    "        self.Q[self.last_state, self.last_action] += self.alpha * (target - self.Q[self.last_state, self.last_action])\n",
    "        \n",
    "        # Remember current state as last for next step\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent(BaseTD0Agent):\n",
    "    def agent_step(self, reward, state):\n",
    "        \n",
    "        # Update rule for Q learning\n",
    "        target = reward + self.gamma * self.Q[state, :].max()\n",
    "        self.Q[self.last_state, self.last_action] += self.alpha * (target - self.Q[self.last_state, self.last_action])\n",
    "        \n",
    "        # Update Policy\n",
    "        self.policy = self.get_e_greedy_policy()\n",
    "        \n",
    "        # Choose new actions:\n",
    "        action = np.random.choice(range(self.n_actions), p = self.policy[state])\n",
    "        \n",
    "        # Remember current state as last for next step\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedSarsaAgent(BaseTD0Agent):\n",
    "    def agent_step(self, reward, state):\n",
    "        \n",
    "        # Update rule for Expected SARSA learning\n",
    "        target = reward + self.gamma * np.dot(self.Q[state, :], self.policy[state, :])\n",
    "        self.Q[self.last_state, self.last_action] += self.alpha * (target - self.Q[self.last_state, self.last_action])\n",
    "        \n",
    "        # Update Policy\n",
    "        self.policy = self.get_e_greedy_policy()\n",
    "        \n",
    "        # Choose new actions:\n",
    "        action = np.random.choice(range(self.n_actions), p = self.policy[state])\n",
    "        \n",
    "        # Remember current state as last for next step\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment():\n",
    "    def exp_init(self, env, agent, agent_info):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        \n",
    "        self.env.env_init()\n",
    "        self.agent.agent_init(agent_info)\n",
    "    \n",
    "    def run_episode(self):\n",
    "        r, s, t = self.env.env_start()\n",
    "        a = self.agent.agent_start(s)\n",
    "\n",
    "        while t == False:\n",
    "            r, s, t = self.env.env_step(a)\n",
    "            if t == False:\n",
    "                a = self.agent.agent_step(r, s)\n",
    "            else: #Termination\n",
    "                self.agent.agent_end(r)\n",
    "        self.agent.agent_cleanup()\n",
    "        self.env.env_cleanup()\n",
    "        \n",
    "    def run_n_episodes(self, n):\n",
    "        for i in range(n):\n",
    "            self.run_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0:  Down\n",
      "State 6:  Down\n",
      "State 12:  Right\n",
      "State 13:  Right\n",
      "State 14:  Up\n",
      "State 8:  Up\n",
      "State 2:  Right\n",
      "State 3:  Right\n",
      "State 4:  Down\n",
      "State 10:  Down\n",
      "State 16:  Down\n",
      "State 22:  Right\n"
     ]
    }
   ],
   "source": [
    "## Sarsa Agent Example\n",
    "env = Environment()\n",
    "\n",
    "agent = SarsaAgent()\n",
    "agent_info = {'gamma': 1, 'alpha': 0.2, 'epsilon': 0.2, 'n_actions': 4, 'n_states': 24}\n",
    "\n",
    "exp = Experiment()\n",
    "\n",
    "exp.exp_init(env = env, agent = agent, agent_info = agent_info)\n",
    "\n",
    "exp.run_n_episodes(150)\n",
    "agent.agent_message('check_policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0:  Down\n",
      "State 6:  Down\n",
      "State 12:  Right\n",
      "State 13:  Right\n",
      "State 14:  Up\n",
      "State 8:  Up\n",
      "State 2:  Right\n",
      "State 3:  Right\n",
      "State 4:  Down\n",
      "State 10:  Down\n",
      "State 16:  Down\n",
      "State 22:  Right\n"
     ]
    }
   ],
   "source": [
    "## Q Agent Example\n",
    "env = Environment()\n",
    "\n",
    "agent = QAgent()\n",
    "agent_info = {'gamma': 1, 'alpha': 0.2, 'epsilon': 0.2, 'n_actions': 4, 'n_states': 24}\n",
    "\n",
    "exp = Experiment()\n",
    "\n",
    "exp.exp_init(env = env, agent = agent, agent_info = agent_info)\n",
    "exp.run_n_episodes(50)\n",
    "agent.agent_message('check_policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0:  Down\n",
      "State 6:  Down\n",
      "State 12:  Right\n",
      "State 13:  Right\n",
      "State 14:  Up\n",
      "State 8:  Up\n",
      "State 2:  Right\n",
      "State 3:  Right\n",
      "State 4:  Down\n",
      "State 10:  Down\n",
      "State 16:  Down\n",
      "State 22:  Right\n"
     ]
    }
   ],
   "source": [
    "## Expected Sarsa Agent Example\n",
    "env = Environment()\n",
    "\n",
    "agent = ExpectedSarsaAgent()\n",
    "agent_info = {'gamma': 1, 'alpha': 0.2, 'epsilon': 0.2, 'n_actions': 4, 'n_states': 24}\n",
    "\n",
    "exp = Experiment()\n",
    "\n",
    "exp.exp_init(env = env, agent = agent, agent_info = agent_info)\n",
    "exp.run_n_episodes(150)\n",
    "agent.agent_message('check_policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
